import streamlit as st
import whisper
from pydub import AudioSegment
import tempfile
import os
from datetime import datetime, timedelta
import pandas as pd
import plotly.graph_objects as go
from collections import Counter
import json
import re

# C·∫•u h√¨nh trang
st.set_page_config(
    page_title="Ph√¢n t√≠ch Ph√°t √Çm Ti·∫øng Anh",
    page_icon="üé§",
    layout="wide",
)

# CSS t√πy ch·ªânh
st.markdown(
    """
<style>
    .stButton>button {
        width: 100%;
    }
    .score-excellent {
        color: #00c853;
        font-size: 48px;
        font-weight: bold;
    }
    .score-good {
        color: #ffd600;
        font-size: 48px;
        font-weight: bold;
    }
    .score-average {
        color: #ff9800;
        font-size: 48px;
        font-weight: bold;
    }
    .score-poor {
        color: #ff5252;
        font-size: 48px;
        font-weight: bold;
    }
    .stat-box {
        background-color: #f0f2f6;
        padding: 20px;
        border-radius: 10px;
        text-align: center;
    }
    .streak-badge {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 15px;
        border-radius: 10px;
        text-align: center;
        font-size: 24px;
        font-weight: bold;
    }
</style>
""",
    unsafe_allow_html=True,
)

# Kh·ªüi t·∫°o session state
if "history" not in st.session_state:
    st.session_state.history = []

if "model" not in st.session_state:
    st.session_state.model = None

if "user_name" not in st.session_state:
    st.session_state.user_name = "H·ªçc vi√™n"

if "daily_goal" not in st.session_state:
    st.session_state.daily_goal = 10

if "model_size" not in st.session_state:
    st.session_state.model_size = "base"


@st.cache_resource
def load_whisper_model(model_size="base"):
    """Load m√¥ h√¨nh Whisper (ch·ªâ load 1 l·∫ßn)"""
    try:
        model = whisper.load_model(model_size)
        return model
    except Exception as e:
        st.error(f"L·ªói khi load Whisper model: {e}")
        return None


# T·ª± ƒë·ªông load model l·∫ßn ƒë·∫ßu
if st.session_state.model is None:
    st.session_state.model = load_whisper_model("base")


def convert_audio_to_wav(audio_file):
    """Chuy·ªÉn ƒë·ªïi file audio v·ªÅ ƒë·ªãnh d·∫°ng WAV chu·∫©n"""
    try:
        audio = AudioSegment.from_file(audio_file)
        audio = audio.set_channels(1)
        audio = audio.set_frame_rate(16000)
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
        audio.export(temp_file.name, format="wav")
        return temp_file.name
    except Exception as e:
        st.error(f"L·ªói chuy·ªÉn ƒë·ªïi audio: {e}")
        return None


def transcribe_audio(audio_path, model):
    """Nh·∫≠n d·∫°ng gi·ªçng n√≥i t·ª´ file audio b·∫±ng Whisper"""
    try:
        result = model.transcribe(audio_path, language="en")
        return result["text"].strip()
    except Exception as e:
        st.error(f"L·ªói nh·∫≠n d·∫°ng gi·ªçng n√≥i: {e}")
        return None


def check_grammar_basic(text):
    """
    Ki·ªÉm tra l·ªói ng·ªØ ph√°p c∆° b·∫£n (m√¥ ph·ªèng)
    Tr·∫£ v·ªÅ ƒëi·ªÉm t·ª´ 0-2
    """
    words = text.lower().split()
    errors = []
    score = 2.0

    # Ki·ªÉm tra Subject-Verb Agreement c∆° b·∫£n
    common_errors = [
        ("i is", "i am"),
        ("he are", "he is"),
        ("she are", "she is"),
        ("they is", "they are"),
        ("we is", "we are"),
        ("i does", "i do"),
        ("he do", "he does"),
        ("she do", "she does"),
    ]

    text_lower = text.lower()
    for wrong, correct in common_errors:
        if wrong in text_lower:
            errors.append(f"L·ªói S-V: '{wrong}' ‚Üí '{correct}'")
            score -= 0.3

    # Ki·ªÉm tra thi·∫øu ƒë·ªông t·ª´ (c√¢u kh√¥ng c√≥ ƒë·ªông t·ª´ ph·ªï bi·∫øn)
    sentences = re.split(r"[.!?]", text)
    basic_verbs = [
        "is",
        "are",
        "am",
        "was",
        "were",
        "have",
        "has",
        "had",
        "do",
        "does",
        "did",
        "can",
        "could",
        "will",
        "would",
        "should",
        "go",
        "get",
        "make",
        "take",
        "see",
        "know",
    ]

    for sent in sentences:
        sent_words = sent.lower().split()
        if len(sent_words) > 3:  # C√¢u ƒë·ªß d√†i
            has_verb = any(verb in sent_words for verb in basic_verbs)
            if not has_verb:
                score -= 0.2

    return max(score, 0), errors


def check_fluency(text):
    """
    ƒê√°nh gi√° ƒë·ªô tr√¥i ch·∫£y v√† t·ª± nhi√™n (d·ª±a tr√™n c·∫•u tr√∫c c√¢u)
    Tr·∫£ v·ªÅ ƒëi·ªÉm t·ª´ 0-2
    """
    words = text.split()
    word_count = len(words)

    if word_count == 0:
        return 0, ["Kh√¥ng c√≥ n·ªôi dung"]

    # ƒê·∫øm s·ªë c√¢u
    sentences = re.split(r"[.!?]+", text)
    sentences = [s.strip() for s in sentences if s.strip()]
    sentence_count = len(sentences)

    # T√≠nh ƒë·ªô d√†i c√¢u trung b√¨nh
    avg_sentence_length = word_count / max(sentence_count, 1)

    score = 1.5  # ƒêi·ªÉm c∆° b·∫£n
    issues = []

    # ƒê·ªô d√†i b√†i n√≥i
    if word_count >= 40:
        score += 0.3
    elif word_count < 20:
        score -= 0.3
        issues.append("B√†i n√≥i qu√° ng·∫Øn")

    # ƒê·ªô d√†i c√¢u (c√¢u qu√° ng·∫Øn ho·∫∑c qu√° d√†i ƒë·ªÅu kh√¥ng t·ªët)
    if 5 <= avg_sentence_length <= 15:
        score += 0.2
    elif avg_sentence_length < 3:
        issues.append("C√¢u qu√° ng·∫Øn, thi·∫øu t·ª± nhi√™n")
        score -= 0.2

    return max(min(score, 2.0), 0), issues


def check_vocabulary(text):
    """
    ƒê√°nh gi√° t·ª´ v·ª±ng (ƒëa d·∫°ng v√† ph√π h·ª£p ch·ªß ƒë·ªÅ)
    Tr·∫£ v·ªÅ ƒëi·ªÉm t·ª´ 0-2
    """
    words = text.lower().split()
    words_clean = [re.sub(r"[^a-z]", "", w) for w in words]
    words_clean = [w for w in words_clean if len(w) > 2]

    if len(words_clean) == 0:
        return 0, ["Kh√¥ng c√≥ t·ª´ v·ª±ng"]

    # T√≠nh ƒë·ªô ƒëa d·∫°ng t·ª´ v·ª±ng
    unique_words = set(words_clean)
    vocab_diversity = len(unique_words) / len(words_clean)

    # ƒê·∫øm t·ª´ ph·ª©c t·∫°p (>= 6 k√Ω t·ª±)
    complex_words = [w for w in words_clean if len(w) >= 6]
    complex_ratio = len(complex_words) / len(words_clean)

    score = 1.0  # ƒêi·ªÉm c∆° b·∫£n
    issues = []

    # ƒêi·ªÉm t·ª´ ƒë·ªô ƒëa d·∫°ng
    if vocab_diversity >= 0.6:
        score += 0.5
    elif vocab_diversity < 0.4:
        issues.append("T·ª´ v·ª±ng b·ªã l·∫∑p l·∫°i nhi·ªÅu")
        score -= 0.2

    # ƒêi·ªÉm t·ª´ ƒë·ªô ph·ª©c t·∫°p
    if complex_ratio >= 0.2:
        score += 0.5
    elif complex_ratio < 0.1:
        issues.append("N√™n s·ª≠ d·ª•ng t·ª´ v·ª±ng ƒëa d·∫°ng h∆°n")
        score -= 0.2

    return max(min(score, 2.0), 0), issues


def check_pronunciation(text, reference_text=None):
    """
    ƒê√°nh gi√° ph√°t √¢m d·ª±a tr√™n:
    1. ƒê·ªô CH√çNH X√ÅC v√† M·∫†CH L·∫†C c·ªßa transcription (n·∫øu kh√¥ng c√≥ reference)
    2. ƒê·ªô T∆Ø∆†NG ƒê·ªíNG v·ªõi c√¢u g·ªëc (n·∫øu c√≥ reference)
    Tr·∫£ v·ªÅ ƒëi·ªÉm t·ª´ 0-2 v√† feedback chi ti·∫øt
    """
    words = text.split()
    word_count = len(words)

    if word_count == 0:
        return 0, ["‚ö†Ô∏è Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c n·ªôi dung"]

    # ==== PH·∫¶N 1: N·∫æU C√ì REFERENCE TEXT - SO S√ÅNH TR·ª∞C TI·∫æP ====
    if reference_text and reference_text.strip():
        return check_pronunciation_with_reference(text, reference_text)

    # ==== PH·∫¶N 2: N·∫æU KH√îNG C√ì REFERENCE - ƒê√ÅNH GI√Å D·ª∞A TR√äN CH·∫§T L∆Ø·ª¢NG ====
    # C√°c pattern b√°o hi·ªáu ph√°t √¢m K√âM (Whisper nh·∫≠n d·∫°ng sai)
    error_indicators = {
        # T·ª´ v√¥ nghƒ©a / ng·∫´u nhi√™n
        "nonsense_words": [
            "hver",
            "ung",
            "hver",
            "isch",
            "artstrom",
            "justarta",
            "matery",
            "hver",
            "ÿßŸÑÿ™",
            "ÿßŸÑ",
        ],
        # Pattern l·∫∑p l·∫°i b·∫•t th∆∞·ªùng
        "repetitive": ["me me", "I I", "you you", "the the the"],
        # T·ª´ qu√° ng·∫Øn kh√¥ng r√µ nghƒ©a
        "unclear_short": ["i", "a", "o", "u", "e"],  # ƒê∆°n l·∫ª, kh√¥ng c√≥ ng·ªØ c·∫£nh
    }

    # Ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng transcription
    issues = []
    penalty = 0.0

    # 1. Ki·ªÉm tra t·ª´ v√¥ nghƒ©a
    nonsense_count = 0
    nonsense_found = []
    text_lower = text.lower()

    for nonsense in error_indicators["nonsense_words"]:
        if nonsense in text_lower:
            nonsense_count += 1
            nonsense_found.append(nonsense)

    if nonsense_count > 0:
        penalty += 0.3 * min(nonsense_count, 3)  # Max -0.9
        issues.append(f"‚ö†Ô∏è Ph√°t hi·ªán {nonsense_count} t·ª´ kh√¥ng r√µ nghƒ©a")

    # 2. Ki·ªÉm tra pattern l·∫∑p l·∫°i b·∫•t th∆∞·ªùng
    repetition_count = 0
    for pattern in error_indicators["repetitive"]:
        if pattern in text_lower:
            repetition_count += 1

    if repetition_count > 0:
        penalty += 0.2 * repetition_count
        issues.append("‚ö†Ô∏è Ph√°t hi·ªán pattern l·∫∑p t·ª´ b·∫•t th∆∞·ªùng")

    # 3. Ki·ªÉm tra t·ª∑ l·ªá t·ª´ qu√° ng·∫Øn (< 3 k√Ω t·ª±) - d·∫•u hi·ªáu n√≥i ng·∫Øt qu√£ng
    short_words = [w for w in words if len(re.sub(r"[^a-zA-Z]", "", w)) < 3]
    short_ratio = len(short_words) / word_count

    if short_ratio > 0.5:  # Qu√° 50% t·ª´ ng·∫Øn
        penalty += 0.3
        issues.append(
            f"‚ö†Ô∏è Qu√° nhi·ªÅu t·ª´ ng·∫Øn ({short_ratio*100:.0f}%) - ph√°t √¢m c√≥ th·ªÉ kh√¥ng r√µ"
        )

    # 4. Ki·ªÉm tra ƒë·ªô d√†i trung b√¨nh c·ªßa t·ª´ (t·ª´ qu√° ng·∫Øn = ph√°t √¢m kh√¥ng r√µ)
    avg_word_length = sum(len(re.sub(r"[^a-zA-Z]", "", w)) for w in words) / word_count

    if avg_word_length < 3.0:
        penalty += 0.2
        issues.append(f"‚ö†Ô∏è T·ª´ trung b√¨nh qu√° ng·∫Øn ({avg_word_length:.1f} k√Ω t·ª±)")

    # 5. Ki·ªÉm tra t√≠nh m·∫°ch l·∫°c c√¢u (c√≥ ƒë·ªông t·ª´, danh t·ª´ c∆° b·∫£n)
    has_basic_structure = any(
        word in text_lower
        for word in ["is", "am", "are", "have", "can", "my", "i", "you", "we"]
    )

    if not has_basic_structure:
        penalty += 0.3
        issues.append("‚ö†Ô∏è Thi·∫øu c·∫•u tr√∫c c√¢u c∆° b·∫£n")

    # 6. Ki·ªÉm tra t·ª∑ l·ªá k√Ω t·ª± s·ªë/ƒë·∫∑c bi·ªát (d·∫•u hi·ªáu Whisper kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c)
    special_char_count = len(re.findall(r"[^a-zA-Z0-9\s\.\,\!\?]", text))
    if special_char_count > word_count * 0.1:  # Qu√° 10%
        penalty += 0.2
        issues.append("‚ö†Ô∏è Ch·ª©a nhi·ªÅu k√Ω t·ª± ƒë·∫∑c bi·ªát (d·∫•u hi·ªáu kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c)")

    # T√≠nh ƒëi·ªÉm pronunciation (2.0 - penalty)
    score = max(2.0 - penalty, 0.0)

    # T·∫°o feedback chi ti·∫øt
    feedback = []
    feedback.append(f"**üìä Ph√¢n t√≠ch Pronunciation:**")
    feedback.append(f"‚Ä¢ T·ªïng s·ªë t·ª´: **{word_count}** t·ª´")
    feedback.append(f"‚Ä¢ ƒê·ªô d√†i t·ª´ TB: **{avg_word_length:.1f}** k√Ω t·ª±")
    feedback.append(f"‚Ä¢ T·ª∑ l·ªá t·ª´ ng·∫Øn: **{short_ratio*100:.0f}%**")
    feedback.append("")

    # Hi·ªÉn th·ªã c√°c v·∫•n ƒë·ªÅ ph√°t hi·ªán ƒë∆∞·ª£c
    if issues:
        feedback.append("**üîç V·∫•n ƒë·ªÅ ph√°t hi·ªán:**")
        for issue in issues:
            feedback.append(f"‚Ä¢ {issue}")
        feedback.append("")

        if nonsense_found:
            feedback.append("**‚ùå T·ª´ kh√¥ng r√µ nghƒ©a:**")
            feedback.append(f"‚Ä¢ {', '.join(nonsense_found[:5])}")
            feedback.append("")

    # ƒê√°nh gi√° t·ªïng quan
    if score >= 1.8:
        feedback.append(
            "**‚úÖ Ph√°t √¢m xu·∫•t s·∫Øc!** Whisper nh·∫≠n d·∫°ng r·∫•t t·ªët, gi·ªçng n√≥i r√µ r√†ng."
        )
    elif score >= 1.5:
        feedback.append(
            "**‚úÖ Ph√°t √¢m t·ªët!** Whisper nh·∫≠n d·∫°ng t·ªët, ch·ªâ m·ªôt v√†i ch·ªó c·∫ßn c·∫£i thi·ªán."
        )
    elif score >= 1.2:
        feedback.append(
            "**üìå Ph√°t √¢m kh√°.** M·ªôt s·ªë t·ª´ ch∆∞a r√µ, c·∫ßn ph√°t √¢m r√µ r√†ng h∆°n."
        )
    elif score >= 0.8:
        feedback.append(
            "**‚ö†Ô∏è Ph√°t √¢m c·∫ßn c·∫£i thi·ªán.** Nhi·ªÅu t·ª´ Whisper nh·∫≠n d·∫°ng kh√¥ng ch√≠nh x√°c."
        )
    else:
        feedback.append(
            "**‚ùå Ph√°t √¢m k√©m.** Whisper g·∫∑p kh√≥ khƒÉn nh·∫≠n d·∫°ng, c·∫ßn luy·ªán t·∫≠p nhi·ªÅu."
        )

    # G·ª£i √Ω c·∫£i thi·ªán
    if score < 1.5:
        feedback.append("")
        feedback.append("**üí° C√°ch c·∫£i thi·ªán:**")
        feedback.append("1. **Ph√°t √¢m r√µ t·ª´ng t·ª´:** N√≥i ch·∫≠m, r√µ r√†ng")
        feedback.append("2. **Kh√¥ng n√≥i ng·∫Øt qu√£ng:** N√≥i tr·ªçn c√¢u, kh√¥ng d·ª´ng gi·ªØa t·ª´")
        feedback.append("3. **Luy·ªán √¢m kh√≥:** /r/, /l/, /th/, /v/, /s/")
        feedback.append(
            "4. **Ghi √¢m v√† nghe l·∫°i:** So s√°nh v·ªõi ph√°t √¢m chu·∫©n (Google Translate, Youglish)"
        )

    return round(score, 1), feedback


def check_pronunciation_with_reference(transcribed, reference):
    """
    So s√°nh transcribed text v·ªõi reference text ƒë·ªÉ ƒë√°nh gi√° ph√°t √¢m
    S·ª≠ d·ª•ng Word Error Rate (WER) v√† ph√¢n t√≠ch chi ti·∫øt
    """

    # Chu·∫©n h√≥a text
    def normalize(text):
        text = text.lower()
        text = re.sub(r"[^a-z\s]", "", text)  # Ch·ªâ gi·ªØ ch·ªØ c√°i v√† space
        return text.split()

    ref_words = normalize(reference)
    trans_words = normalize(transcribed)

    if len(ref_words) == 0:
        return 0, ["‚ö†Ô∏è C√¢u tham chi·∫øu kh√¥ng h·ª£p l·ªá"]

    # T√≠nh Word Error Rate (WER) b·∫±ng Levenshtein Distance
    def levenshtein_distance(s1, s2):
        if len(s1) < len(s2):
            return levenshtein_distance(s2, s1)
        if len(s2) == 0:
            return len(s1)
        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        return previous_row[-1]

    # Ph√¢n t√≠ch chi ti·∫øt t·ª´ng t·ª´
    correct_words = 0
    missing_words = []
    extra_words = []

    # So s√°nh t·ª´ng t·ª´ (simple alignment)
    ref_set = set(ref_words)
    trans_set = set(trans_words)

    # T·ª´ ƒë√∫ng: c√≥ trong c·∫£ 2
    matched_ref = []
    matched_trans = []

    for word in ref_words:
        if word in trans_set and word not in matched_ref:
            correct_words += 1
            matched_ref.append(word)
            matched_trans.append(word)

    # T·ª´ thi·∫øu: c√≥ trong ref nh∆∞ng kh√¥ng kh·ªõp
    for word in ref_words:
        if word not in matched_ref:
            missing_words.append(word)

    # T·ª´ th·ª´a: c√≥ trong trans nh∆∞ng kh√¥ng kh·ªõp (CH·ªà ƒê·ªÇ HI·ªÇN TH·ªä, KH√îNG TR·ª™ ƒêI·ªÇM)
    for word in trans_words:
        if word not in matched_trans and word not in ref_set:
            extra_words.append(word)

    # T√≠nh accuracy CH·ªà d·ª±a tr√™n t·ª´ ƒê√öNG v√† THI·∫æU (b·ªè qua t·ª´ th·ª´a)
    # C√¥ng th·ª©c: Accuracy = (T·ª´ ƒë√∫ng) / (T·ªïng t·ª´ g·ªëc)
    accuracy = correct_words / len(ref_words)
    accuracy_percent = accuracy * 100

    # T√≠nh l·ªói ch·ªâ t·ª´ missing words
    error_rate = len(missing_words) / len(ref_words)

    # T√≠nh ƒëi·ªÉm pronunciation d·ª±a tr√™n accuracy
    if accuracy >= 0.95:
        score = 2.0
        grade = "Xu·∫•t s·∫Øc"
        emoji = "üü¢"
    elif accuracy >= 0.90:
        score = 1.8
        grade = "R·∫•t t·ªët"
        emoji = "üü¢"
    elif accuracy >= 0.80:
        score = 1.5
        grade = "Kh√° t·ªët"
        emoji = "üü°"
    elif accuracy >= 0.70:
        score = 1.2
        grade = "Trung b√¨nh kh√°"
        emoji = "üü°"
    elif accuracy >= 0.60:
        score = 0.9
        grade = "Trung b√¨nh"
        emoji = "üü†"
    elif accuracy >= 0.50:
        score = 0.6
        grade = "Y·∫øu"
        emoji = "üü†"
    else:
        score = 0.3
        grade = "K√©m"
        emoji = "üî¥"

    # T·∫°o feedback chi ti·∫øt
    feedback = []
    feedback.append(f"**üìä Ph√¢n t√≠ch So s√°nh v·ªõi C√¢u G·ªëc:**")
    feedback.append(f"‚Ä¢ **ƒê·ªô ch√≠nh x√°c: {emoji} {accuracy_percent:.1f}%** ({grade})")
    feedback.append(f"‚Ä¢ C√¢u g·ªëc: **{len(ref_words)}** t·ª´")
    feedback.append(f"‚Ä¢ B·∫°n n√≥i: **{len(trans_words)}** t·ª´")
    feedback.append(f"‚Ä¢ T·ª´ ph√°t √¢m ƒë√∫ng: **{correct_words}/{len(ref_words)}** t·ª´")
    feedback.append(f"‚Ä¢ T·ª´ thi·∫øu/sai: **{len(missing_words)}** t·ª´")
    if extra_words:
        feedback.append(
            f"‚Ä¢ T·ª´ m·ªü r·ªông th√™m: **{len(extra_words)}** t·ª´ *(kh√¥ng tr·ª´ ƒëi·ªÉm)*"
        )
    feedback.append("")

    # Hi·ªÉn th·ªã t·ª´ thi·∫øu chi ti·∫øt
    if missing_words:
        feedback.append(f"**‚ùå T·ª´ THI·∫æU/SAI ({len(missing_words)} t·ª´):**")
        missing_unique = list(set(missing_words))[:15]
        feedback.append(f"‚Ä¢ {', '.join(missing_unique)}")
        feedback.append("")

    if extra_words:
        feedback.append(f"**‚ûï T·ª´ M·ªû R·ªòNG TH√äM ({len(extra_words)} t·ª´):**")
        extra_unique = list(set(extra_words))[:15]
        feedback.append(f"‚Ä¢ {', '.join(extra_unique)}")
        feedback.append(f"‚Ä¢ *(Kh√¥ng b·ªã tr·ª´ ƒëi·ªÉm - Khuy·∫øn kh√≠ch m·ªü r·ªông √Ω!)*")
        feedback.append("")

    # ƒê√°nh gi√° t·ªïng quan
    if accuracy >= 0.90:
        feedback.append(
            "**‚úÖ Xu·∫•t s·∫Øc!** Ph√°t √¢m r·∫•t chu·∫©n, ph·∫ßn l·ªõn t·ª´ trong c√¢u g·ªëc ƒë·ªÅu ƒë√∫ng."
        )
    elif accuracy >= 0.80:
        feedback.append(
            "**‚úÖ R·∫•t t·ªët!** Ph·∫ßn l·ªõn t·ª´ ph√°t √¢m chu·∫©n, ch·ªâ thi·∫øu/sai m·ªôt v√†i t·ª´."
        )
    elif accuracy >= 0.70:
        feedback.append("**üìå Kh√° t·ªët.** M·ªôt s·ªë t·ª´ trong c√¢u g·ªëc c·∫ßn c·∫£i thi·ªán.")
    elif accuracy >= 0.60:
        feedback.append("**‚ö†Ô∏è Trung b√¨nh.** Nhi·ªÅu t·ª´ trong c√¢u g·ªëc b·ªã thi·∫øu/sai.")
    else:
        feedback.append(
            "**‚ùå C·∫ßn c·∫£i thi·ªán.** Ph·∫ßn l·ªõn t·ª´ trong c√¢u g·ªëc ch∆∞a ph√°t √¢m ƒë√∫ng."
        )

    # G·ª£i √Ω c·∫£i thi·ªán
    if accuracy < 0.85:
        feedback.append("")
        feedback.append("**üí° G·ª£i √Ω c·∫£i thi·ªán:**")
        feedback.append("1. **T·∫≠p trung v√†o t·ª´ THI·∫æU/SAI** ·ªü tr√™n")
        feedback.append("2. **ƒê·ªçc ch·∫≠m t·ª´ng t·ª´** trong c√¢u g·ªëc")
        feedback.append("3. **Nghe v√† nh·∫Øc l·∫°i** nhi·ªÅu l·∫ßn")
        feedback.append("4. **So s√°nh ghi √¢m** c·ªßa b·∫°n v·ªõi ph√°t √¢m chu·∫©n")

    if extra_words and accuracy >= 0.75:
        feedback.append("")
        feedback.append("**üåü ƒêi·ªÉm c·ªông:**")
        feedback.append("‚Ä¢ B·∫°n ƒë√£ m·ªü r·ªông √Ω r·∫•t t·ªët! Ti·∫øp t·ª•c ph√°t tri·ªÉn k·ªπ nƒÉng n√†y!")

    return round(score, 1), feedback


def check_communication(text):
    """
    ƒê√°nh gi√° kh·∫£ nƒÉng truy·ªÅn ƒë·∫°t √Ω (c√≥ tr·∫£ l·ªùi ƒë√∫ng c√¢u h·ªèi, logic, r√µ r√†ng)
    Tr·∫£ v·ªÅ ƒëi·ªÉm t·ª´ 0-2
    """
    words = text.lower().split()
    word_count = len(words)

    score = 1.0  # ƒêi·ªÉm c∆° b·∫£n
    issues = []

    # Ki·ªÉm tra ƒë·ªô d√†i (c√≥ ƒë·ªß n·ªôi dung kh√¥ng)
    if word_count >= 30:
        score += 0.5
    elif word_count < 15:
        score -= 0.3
        issues.append("C√¢u tr·∫£ l·ªùi qu√° ng·∫Øn, thi·∫øu chi ti·∫øt")

    # Ki·ªÉm tra c√≥ t·ª´ n·ªëi (because, and, so, but) - th·ªÉ hi·ªán logic
    connectors = ["because", "and", "so", "but", "also", "however", "therefore"]
    has_connector = any(conn in words for conn in connectors)
    if has_connector:
        score += 0.3
    else:
        issues.append("N√™n d√πng t·ª´ n·ªëi ƒë·ªÉ li√™n k·∫øt √Ω")

    # Ki·ªÉm tra c√≥ c√¢u gi·ªõi thi·ªáu (my name, i am, i like, my favorite)
    intro_phrases = ["my name", "i am", "my favorite", "i like", "i love"]
    has_intro = any(phrase in text.lower() for phrase in intro_phrases)
    if has_intro:
        score += 0.2

    return max(min(score, 2.0), 0), issues


def analyze_speech(transcribed_text, audio_path=None, reference_text=None):
    """
    Ph√¢n t√≠ch b√†i n√≥i theo 5 ti√™u ch√≠ (m·ªói ti√™u ch√≠ /2 ƒëi·ªÉm, t·ªïng /10)
    """
    if not transcribed_text or len(transcribed_text.strip()) == 0:
        return 0, ["‚ö†Ô∏è Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c n·ªôi dung. Vui l√≤ng th·ª≠ l·∫°i."], {}

    # Ch·∫•m t·ª´ng ti√™u ch√≠
    pronunciation_score, pronunciation_issues = check_pronunciation(
        transcribed_text, reference_text
    )
    fluency_score, fluency_issues = check_fluency(transcribed_text)
    grammar_score, grammar_issues = check_grammar_basic(transcribed_text)
    vocabulary_score, vocabulary_issues = check_vocabulary(transcribed_text)
    communication_score, communication_issues = check_communication(transcribed_text)

    # T·ªïng ƒëi·ªÉm /10
    total_score = (
        pronunciation_score
        + fluency_score
        + grammar_score
        + vocabulary_score
        + communication_score
    )

    # Chuy·ªÉn sang thang ƒëi·ªÉm 100
    final_score_100 = (total_score / 10) * 100

    # T·∫°o breakdown chi ti·∫øt
    breakdown = {
        "Pronunciation": pronunciation_score,
        "Fluency": fluency_score,
        "Grammar": grammar_score,
        "Vocabulary": vocabulary_score,
        "Communication": communication_score,
        "Total": total_score,
    }

    # T·∫°o feedback chi ti·∫øt
    feedback = []

    feedback.append(
        f"üìä **T·ªîNG ƒêI·ªÇM: {total_score:.1f}/10** ({final_score_100:.0f}/100)"
    )
    feedback.append("---")

    # 1. Pronunciation
    feedback.append(f"üó£Ô∏è **1. Pronunciation (Ph√°t √¢m): {pronunciation_score:.1f}/2**")
    feedback.append("")
    if pronunciation_issues:
        for issue in pronunciation_issues:
            feedback.append(f"{issue}")
    feedback.append("")

    # 2. Fluency
    feedback.append(f"üéµ **2. Fluency (ƒê·ªô tr√¥i ch·∫£y): {fluency_score:.1f}/2**")
    if fluency_issues:
        for issue in fluency_issues:
            feedback.append(f"   ‚Ä¢ {issue}")
    else:
        feedback.append("   ‚Ä¢ N√≥i kh√° t·ª± nhi√™n v√† m·∫°ch l·∫°c")
    feedback.append("")

    # 3. Grammar
    feedback.append(f"üìù **3. Grammar (Ng·ªØ ph√°p): {grammar_score:.1f}/2**")
    if grammar_issues:
        for issue in grammar_issues:
            feedback.append(f"   ‚Ä¢ {issue}")
    else:
        feedback.append("   ‚Ä¢ Ng·ªØ ph√°p ch√≠nh x√°c")
    feedback.append("")

    # 4. Vocabulary
    feedback.append(f"üìö **4. Vocabulary (T·ª´ v·ª±ng): {vocabulary_score:.1f}/2**")
    if vocabulary_issues:
        for issue in vocabulary_issues:
            feedback.append(f"   ‚Ä¢ {issue}")
    else:
        feedback.append("   ‚Ä¢ T·ª´ v·ª±ng ƒëa d·∫°ng v√† ph√π h·ª£p")
    feedback.append("")

    # 5. Communication
    feedback.append(f"üí¨ **5. Communication (Giao ti·∫øp): {communication_score:.1f}/2**")
    if communication_issues:
        for issue in communication_issues:
            feedback.append(f"   ‚Ä¢ {issue}")
    else:
        feedback.append("   ‚Ä¢ Truy·ªÅn ƒë·∫°t √Ω r√µ r√†ng v√† logic")

    return round(final_score_100, 1), feedback, breakdown


def save_result_to_history(topic, transcribed, score, wrong_words, breakdown=None):
    """L∆∞u k·∫øt qu·∫£ v√†o l·ªãch s·ª≠"""
    result = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "date": datetime.now().strftime("%Y-%m-%d"),
        "time": datetime.now().strftime("%H:%M:%S"),
        "reference": topic,
        "transcribed": transcribed,
        "score": score,
        "wrong_words": wrong_words,
        "word_count": len(transcribed.split()),
        "user": st.session_state.user_name,
        "mode": "B√†i n√≥i t·ª± do",
        "breakdown": breakdown if breakdown else {},
    }
    st.session_state.history.append(result)


def export_history_to_csv():
    """Xu·∫•t l·ªãch s·ª≠ ra CSV"""
    if not st.session_state.history:
        return None

    df = pd.DataFrame(st.session_state.history)
    return df.to_csv(index=False).encode("utf-8")


def export_history_to_json():
    """Xu·∫•t l·ªãch s·ª≠ ra JSON (ƒë·ªÉ backup)"""
    if not st.session_state.history:
        return None

    data = {
        "export_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "user_name": st.session_state.user_name,
        "history": st.session_state.history,
    }
    return json.dumps(data, indent=2, ensure_ascii=False).encode("utf-8")


def import_history_from_json(json_file):
    """Nh·∫≠p l·ªãch s·ª≠ t·ª´ JSON"""
    try:
        data = json.loads(json_file.read())
        st.session_state.history = data.get("history", [])
        return True
    except Exception as e:
        st.error(f"L·ªói import: {e}")
        return False


def get_statistics():
    """T√≠nh to√°n th·ªëng k√™"""
    if not st.session_state.history:
        return None

    df = pd.DataFrame(st.session_state.history)

    stats = {
        "total_attempts": len(df),
        "avg_score": df["score"].mean(),
        "max_score": df["score"].max(),
        "min_score": df["score"].min(),
        "today_attempts": len(df[df["date"] == datetime.now().strftime("%Y-%m-%d")]),
        "excellent_count": len(df[df["score"] >= 90]),
        "good_count": len(df[(df["score"] >= 75) & (df["score"] < 90)]),
        "average_count": len(df[(df["score"] >= 60) & (df["score"] < 75)]),
        "poor_count": len(df[df["score"] < 60]),
    }

    return stats


def calculate_streak():
    """T√≠nh chu·ªói ng√†y luy·ªán t·∫≠p li√™n t·ª•c"""
    if not st.session_state.history:
        return 0

    df = pd.DataFrame(st.session_state.history)
    unique_dates = sorted(df["date"].unique(), reverse=True)

    if not unique_dates:
        return 0

    today = datetime.now().strftime("%Y-%m-%d")

    if unique_dates[0] != today:
        yesterday = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
        if unique_dates[0] != yesterday:
            return 0

    streak = 1
    for i in range(len(unique_dates) - 1):
        current = datetime.strptime(unique_dates[i], "%Y-%m-%d")
        next_date = datetime.strptime(unique_dates[i + 1], "%Y-%m-%d")

        if (current - next_date).days == 1:
            streak += 1
        else:
            break

    return streak


def create_progress_chart():
    """T·∫°o bi·ªÉu ƒë·ªì ti·∫øn ƒë·ªô"""
    if not st.session_state.history:
        return None

    df = pd.DataFrame(st.session_state.history)
    df["attempt_number"] = range(1, len(df) + 1)

    fig = go.Figure()

    fig.add_trace(
        go.Scatter(
            x=df["attempt_number"],
            y=df["score"],
            mode="lines+markers",
            name="ƒêi·ªÉm s·ªë",
            line=dict(color="#1f77b4", width=3),
            marker=dict(size=8),
        )
    )

    fig.add_hline(
        y=90, line_dash="dash", line_color="green", annotation_text="Xu·∫•t s·∫Øc (90)"
    )
    fig.add_hline(
        y=75, line_dash="dash", line_color="orange", annotation_text="Kh√° (75)"
    )
    fig.add_hline(
        y=60, line_dash="dash", line_color="red", annotation_text="Trung b√¨nh (60)"
    )

    fig.update_layout(
        title="üìà Bi·ªÉu ƒë·ªì ti·∫øn ƒë·ªô h·ªçc t·∫≠p",
        xaxis_title="L·∫ßn ch·∫•m",
        yaxis_title="ƒêi·ªÉm s·ªë",
        yaxis_range=[0, 105],
        hovermode="x unified",
    )

    return fig


def create_score_distribution():
    """T·∫°o bi·ªÉu ƒë·ªì ph√¢n b·ªë ƒëi·ªÉm"""
    if not st.session_state.history:
        return None

    df = pd.DataFrame(st.session_state.history)

    fig = go.Figure()

    fig.add_trace(
        go.Histogram(x=df["score"], nbinsx=20, marker_color="#1f77b4", opacity=0.7)
    )

    fig.update_layout(
        title="üìä Ph√¢n b·ªë ƒëi·ªÉm s·ªë",
        xaxis_title="ƒêi·ªÉm s·ªë",
        yaxis_title="S·ªë l·∫ßn",
        bargap=0.1,
    )

    return fig


def create_weekly_chart():
    """Bi·ªÉu ƒë·ªì theo tu·∫ßn"""
    if not st.session_state.history:
        return None

    df = pd.DataFrame(st.session_state.history)
    df["date"] = pd.to_datetime(df["date"])

    end_date = datetime.now()
    start_date = end_date - timedelta(days=6)

    date_range = pd.date_range(start=start_date, end=end_date)
    daily_stats = []

    for date in date_range:
        date_str = date.strftime("%Y-%m-%d")
        day_data = df[df["date"] == date_str]

        if len(day_data) > 0:
            daily_stats.append(
                {
                    "date": date.strftime("%d/%m"),
                    "count": len(day_data),
                    "avg_score": day_data["score"].mean(),
                }
            )
        else:
            daily_stats.append(
                {"date": date.strftime("%d/%m"), "count": 0, "avg_score": 0}
            )

    stats_df = pd.DataFrame(daily_stats)

    fig = go.Figure()

    fig.add_trace(
        go.Bar(
            x=stats_df["date"],
            y=stats_df["count"],
            name="S·ªë l·∫ßn luy·ªán",
            marker_color="#1f77b4",
        )
    )

    fig.update_layout(
        title="üìÖ Ho·∫°t ƒë·ªông 7 ng√†y g·∫ßn nh·∫•t",
        xaxis_title="Ng√†y",
        yaxis_title="S·ªë l·∫ßn luy·ªán t·∫≠p",
    )

    return fig


# ========== GIAO DI·ªÜN CH√çNH ==========

# Header v·ªõi tabs
tab1, tab2, tab3 = st.tabs(["üé§ Ph√¢n t√≠ch B√†i n√≥i", "üìä Th·ªëng k√™", "‚öôÔ∏è C√†i ƒë·∫∑t"])

# TAB 1: PH√ÇN T√çCH B√ÄI N√ìI
with tab1:
    st.title("üé§ PH√ÇN T√çCH PH√ÅT √ÇM B√ÄI N√ìI T·ª∞ DO")

    # Th√¥ng tin ng∆∞·ªùi d√πng v√† streak
    col_info1, col_info2, col_info3, col_info4 = st.columns(4)

    with col_info1:
        st.markdown(f"**üë§ H·ªçc vi√™n:** {st.session_state.user_name}")

    with col_info2:
        streak = calculate_streak()
        if streak > 0:
            st.markdown(f"**üî• Streak:** {streak} ng√†y")

    with col_info3:
        stats = get_statistics()
        if stats:
            st.markdown(
                f"**üìà H√¥m nay:** {stats['today_attempts']}/{st.session_state.daily_goal} l·∫ßn"
            )
            progress = min(stats["today_attempts"] / st.session_state.daily_goal, 1.0)
            st.progress(progress)

    with col_info4:
        if stats:
            st.markdown(f"**‚≠ê ƒêi·ªÉm TB:** {stats['avg_score']:.1f}/100")

    st.divider()

    # Ph·∫ßn nh·∫≠p ch·ªß ƒë·ªÅ
    st.subheader("üìù B∆∞·ªõc 1: Nh·∫≠p ch·ªß ƒë·ªÅ/ƒë·ªÅ b√†i")

    # TH√äM COLUMNS
    col_topic1, col_topic2 = st.columns([2, 1])

    with col_topic1:
        topic_input = st.text_input(
            "Ch·ªß ƒë·ªÅ b√†i n√≥i c·ªßa b·∫°n:",
            value="",
            placeholder="V√≠ d·ª•: My favorite hobby, My hometown, A memorable trip...",
            help="Nh·∫≠p ch·ªß ƒë·ªÅ m√† b·∫°n s·∫Ω n√≥i v·ªÅ",
        )

    with col_topic2:
        use_reference = st.checkbox(
            "üìã C√≥ c√¢u m·∫´u?", help="B·∫≠t n·∫øu b·∫°n c√≥ c√¢u g·ªëc c·∫ßn ƒë·ªçc theo", value=False
        )

    # TH√äM PH·∫¶N REFERENCE TEXT
    reference_text = None
    if use_reference:
        reference_text = st.text_area(
            "üìù Nh·∫≠p c√¢u g·ªëc (reference):",
            value="",
            placeholder="V√≠ d·ª•: Hello everyone, my name is Ivy. I'm a student...",
            help="Nh·∫≠p c√¢u g·ªëc m√† b·∫°n c·∫ßn ƒë·ªçc theo",
            height=100,
        )
        if reference_text:
            st.info(f"üìå C√¢u g·ªëc: **{reference_text[:100]}...**")
    elif topic_input:
        st.info(f"üìå Ch·ªß ƒë·ªÅ: **{topic_input}**")

    st.divider()

    # Ph·∫ßn ghi √¢m v√† upload
    col1, col2 = st.columns(2)

    with col1:
        st.subheader("üéôÔ∏è B∆∞·ªõc 2a: Ghi √¢m tr·ª±c ti·∫øp")
        audio_recording = st.audio_input("Nh·∫•n ƒë·ªÉ ghi √¢m:")

    with col2:
        st.subheader("üì§ B∆∞·ªõc 2b: Ho·∫∑c upload file audio")
        uploaded_file = st.file_uploader(
            "Ch·ªçn file audio:",
            type=["wav", "mp3", "m4a", "ogg", "flac"],
            help="H·ªó tr·ª£ c√°c ƒë·ªãnh d·∫°ng: WAV, MP3, M4A, OGG, FLAC",
        )

    st.divider()

    # N√∫t ph√¢n t√≠ch
    st.subheader("üéØ B∆∞·ªõc 3: Ph√¢n t√≠ch b√†i n√≥i")

    if st.button("üîç Ph√¢n t√≠ch ph√°t √¢m", type="primary", use_container_width=True):

        if not topic_input:
            st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p ch·ªß ƒë·ªÅ b√†i n√≥i!")
        elif not audio_recording and not uploaded_file:
            st.error("‚ö†Ô∏è Vui l√≤ng ghi √¢m ho·∫∑c upload file audio!")
        elif not st.session_state.model:
            st.error("‚ö†Ô∏è Model ch∆∞a ƒë∆∞·ª£c load. Vui l√≤ng load trong tab C√†i ƒë·∫∑t.")
        else:
            with st.spinner("üîÑ ƒêang x·ª≠ l√Ω √¢m thanh..."):
                try:
                    audio_source = uploaded_file if uploaded_file else audio_recording
                    wav_path = convert_audio_to_wav(audio_source)

                    if wav_path:
                        with st.spinner("üéß ƒêang nh·∫≠n d·∫°ng gi·ªçng n√≥i..."):
                            transcribed_text = transcribe_audio(
                                wav_path, st.session_state.model
                            )

                        try:
                            os.unlink(wav_path)
                        except:
                            pass

                        if transcribed_text:
                            score, feedback, breakdown = analyze_speech(
                                transcribed_text, wav_path, reference_text
                            )
                            save_result_to_history(
                                topic_input,
                                transcribed_text,
                                score,
                                feedback,
                                breakdown,
                            )

                            st.success("‚úÖ Ph√¢n t√≠ch ho√†n t·∫•t!")
                            st.balloons()

                            st.divider()

                            # K·∫øt qu·∫£
                            col_result1, col_result2 = st.columns(2)

                            with col_result1:
                                st.markdown("**üìå Ch·ªß ƒë·ªÅ:**")
                                st.info(topic_input)

                            with col_result2:
                                st.markdown("**üó£Ô∏è N·ªôi dung b·∫°n ƒë√£ n√≥i:**")
                                st.info(transcribed_text)

                            # ƒêi·ªÉm s·ªë t·ªïng quan
                            st.markdown("### üéØ K·∫øt qu·∫£ ch·∫•m ƒëi·ªÉm")

                            # Hi·ªÉn th·ªã ƒëi·ªÉm chi ti·∫øt theo 5 ti√™u ch√≠
                            col_breakdown = st.columns(6)

                            criteria_emojis = ["üó£Ô∏è", "üéµ", "üìù", "üìö", "üí¨", "‚≠ê"]
                            criteria_names = [
                                "Pronunciation",
                                "Fluency",
                                "Grammar",
                                "Vocabulary",
                                "Communication",
                                "Total",
                            ]

                            for i, (emoji, name) in enumerate(
                                zip(criteria_emojis, criteria_names)
                            ):
                                with col_breakdown[i]:
                                    if name == "Total":
                                        st.metric(
                                            f"{emoji} {name}",
                                            f"{breakdown.get('Total', 0):.1f}/10",
                                            help="T·ªïng ƒëi·ªÉm",
                                        )
                                    else:
                                        st.metric(
                                            f"{emoji} {name[:4]}.",
                                            f"{breakdown.get(name, 0):.1f}/2",
                                            help=name,
                                        )

                            st.divider()

                            # ƒêi·ªÉm s·ªë l·ªõn v·ªõi m√†u s·∫Øc (chuy·ªÉn sang thang 100 ƒë·ªÉ hi·ªÉn th·ªã)
                            if score >= 90:
                                score_class = "score-excellent"
                                grade = "Xu·∫•t s·∫Øc"
                                emoji = "üü¢"
                            elif score >= 75:
                                score_class = "score-good"
                                grade = "Kh√°"
                                emoji = "üü°"
                            elif score >= 60:
                                score_class = "score-average"
                                grade = "Trung b√¨nh"
                                emoji = "üü†"
                            else:
                                score_class = "score-poor"
                                grade = "C·∫ßn c·∫£i thi·ªán"
                                emoji = "üî¥"

                            col_score1, col_score2 = st.columns([1, 2])

                            with col_score1:
                                st.markdown(
                                    f'<div class="{score_class}">{emoji} {score:.0f}/100</div>',
                                    unsafe_allow_html=True,
                                )

                            with col_score2:
                                st.markdown(f"### {grade}")
                                st.progress(score / 100)

                                if score >= 90:
                                    st.success(
                                        "üéâ Xu·∫•t s·∫Øc! Ph√°t √¢m v√† n·ªôi dung r·∫•t t·ªët!"
                                    )
                                elif score >= 75:
                                    st.info(
                                        "üëç T·ªët l·∫Øm! Ti·∫øp t·ª•c luy·ªán t·∫≠p ƒë·ªÉ ho√†n thi·ªán h∆°n."
                                    )
                                elif score >= 60:
                                    st.warning(
                                        "üí™ Kh√° ·ªïn! H√£y ch√∫ √Ω c√°c g√≥p √Ω ph√≠a d∆∞·ªõi."
                                    )
                                else:
                                    st.error(
                                        "üìö C·∫ßn c·∫£i thi·ªán. H√£y luy·ªán t·∫≠p th√™m v·ªÅ ph√°t √¢m v√† ƒë·ªô d√†i b√†i n√≥i."
                                    )

                            # Feedback chi ti·∫øt
                            st.markdown("### üí° Ph·∫£n h·ªìi chi ti·∫øt")

                            # Hi·ªÉn th·ªã feedback d·∫°ng markdown
                            feedback_text = "\n".join(feedback)
                            st.markdown(feedback_text)

                            # G·ª£i √Ω luy·ªán t·∫≠p
                            with st.expander("üìñ G·ª£i √Ω c·∫£i thi·ªán cho t·ª´ng ti√™u ch√≠"):
                                st.markdown(
                                    """
                                **üó£Ô∏è Pronunciation (Ph√°t √¢m):**
                                - Nghe v√† b·∫Øt ch∆∞·ªõc ng∆∞·ªùi b·∫£n ng·ªØ (BBC Learning English, VOA)
                                - T·∫≠p ph√°t √¢m c√°c √¢m kh√≥: /Œ∏/, /√∞/, /r/, /l/
                                - Ghi √¢m v√† so s√°nh v·ªõi b·∫£n g·ªëc
                                
                                **üéµ Fluency (ƒê·ªô tr√¥i ch·∫£y):**
                                - Luy·ªán n√≥i kh√¥ng ng·∫Øt qu√£ng 1-2 ph√∫t
                                - Gi·∫£m "uh", "um" b·∫±ng c√°ch suy nghƒ© tr∆∞·ªõc khi n√≥i
                                - ƒê·ªçc to 10-15 ph√∫t m·ªói ng√†y
                                
                                **üìù Grammar (Ng·ªØ ph√°p):**
                                - H·ªçc thu·ªôc c√°c th√¨ c∆° b·∫£n (hi·ªán t·∫°i, qu√° kh·ª©, t∆∞∆°ng lai)
                                - Ch√∫ √Ω Subject-Verb Agreement (I am, He is, They are)
                                - L√†m b√†i t·∫≠p ng·ªØ ph√°p tr√™n app (Duolingo, Grammarly)
                                
                                **üìö Vocabulary (T·ª´ v·ª±ng):**
                                - H·ªçc 10 t·ª´ m·ªõi m·ªói ng√†y theo ch·ªß ƒë·ªÅ
                                - S·ª≠ d·ª•ng t·ª´ v·ª±ng ƒëa d·∫°ng, tr√°nh l·∫∑p l·∫°i
                                - ƒê·ªçc s√°ch/b√°o ti·∫øng Anh ƒë·ªÉ m·ªü r·ªông v·ªën t·ª´
                                
                                **üí¨ Communication (Giao ti·∫øp):**
                                - Tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß c√¢u h·ªèi v·ªõi l√Ω do v√† v√≠ d·ª•
                                - S·ª≠ d·ª•ng t·ª´ n·ªëi: because, and, so, but
                                - T·ªï ch·ª©c √Ω: Introduction ‚Üí Main idea ‚Üí Example ‚Üí Conclusion
                                """
                                )

                        else:
                            st.error(
                                "‚ùå Kh√¥ng th·ªÉ nh·∫≠n d·∫°ng gi·ªçng n√≥i. Vui l√≤ng th·ª≠ l·∫°i."
                            )
                    else:
                        st.error("‚ùå Kh√¥ng th·ªÉ x·ª≠ l√Ω file audio.")

                except Exception as e:
                    st.error(f"‚ùå L·ªói: {e}")

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ g·∫ßn ƒë√¢y
    if st.session_state.history:
        st.divider()
        st.subheader("üìú L·ªãch s·ª≠ g·∫ßn ƒë√¢y (5 l·∫ßn cu·ªëi)")

        for item in reversed(st.session_state.history[-5:]):
            with st.expander(f"‚è∞ {item['timestamp']} - ƒêi·ªÉm: {item['score']}/100"):
                col_h1, col_h2 = st.columns(2)
                with col_h1:
                    st.write(f"**üìå Ch·ªß ƒë·ªÅ:** {item['reference']}")
                    st.write(f"**üó£Ô∏è N·ªôi dung:** {item['transcribed'][:100]}...")
                with col_h2:
                    st.metric("ƒêi·ªÉm s·ªë", f"{item['score']}/100")
                    st.write(f"**üë§ Ng∆∞·ªùi d√πng:** {item['user']}")
                    st.write(f"**üìù S·ªë t·ª´:** {item['word_count']}")

                # Hi·ªÉn th·ªã breakdown n·∫øu c√≥
                if "breakdown" in item and item["breakdown"]:
                    st.write("**üìä Chi ti·∫øt ƒëi·ªÉm:**")
                    breakdown = item["breakdown"]
                    col_b = st.columns(5)
                    labels = [
                        "Pronunciation",
                        "Fluency",
                        "Grammar",
                        "Vocabulary",
                        "Communication",
                    ]
                    for idx, label in enumerate(labels):
                        if label in breakdown:
                            with col_b[idx]:
                                st.metric(label[:6], f"{breakdown[label]:.1f}/2")

# TAB 2: TH·ªêNG K√ä
with tab2:
    st.title("üìä Th·ªëng k√™ v√† Ph√¢n t√≠ch")

    stats = get_statistics()

    if stats and stats["total_attempts"] > 0:
        # Hi·ªÉn th·ªã Streak
        streak = calculate_streak()
        if streak > 0:
            st.markdown(
                f"""
            <div class="streak-badge">
                üî• Streak: {streak} ng√†y li√™n t·ª•c!
            </div>
            """,
                unsafe_allow_html=True,
            )
            st.markdown("")

        # T·ªïng quan
        st.subheader("üìà T·ªïng quan")

        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric("T·ªïng s·ªë l·∫ßn ph√¢n t√≠ch", stats["total_attempts"])

        with col2:
            st.metric("ƒêi·ªÉm trung b√¨nh", f"{stats['avg_score']:.1f}/100")

        with col3:
            st.metric("ƒêi·ªÉm cao nh·∫•t", f"{stats['max_score']}/100")

        with col4:
            st.metric("H√¥m nay", f"{stats['today_attempts']} l·∫ßn")

        st.divider()

        # Bi·ªÉu ƒë·ªì
        col_chart1, col_chart2 = st.columns(2)

        with col_chart1:
            progress_chart = create_progress_chart()
            if progress_chart:
                st.plotly_chart(progress_chart, use_container_width=True)

        with col_chart2:
            dist_chart = create_score_distribution()
            if dist_chart:
                st.plotly_chart(dist_chart, use_container_width=True)

        st.divider()

        # Bi·ªÉu ƒë·ªì ho·∫°t ƒë·ªông 7 ng√†y
        weekly_chart = create_weekly_chart()
        if weekly_chart:
            st.plotly_chart(weekly_chart, use_container_width=True)

        st.divider()

        # Ph√¢n lo·∫°i ƒëi·ªÉm
        st.subheader("üéØ Ph√¢n lo·∫°i k·∫øt qu·∫£")

        col_cat1, col_cat2, col_cat3, col_cat4 = st.columns(4)

        with col_cat1:
            st.markdown(
                f"""
            <div class="stat-box" style="border-left: 4px solid #00c853;">
                <h3>üü¢ Xu·∫•t s·∫Øc</h3>
                <h2>{stats['excellent_count']}</h2>
                <p>‚â• 90 ƒëi·ªÉm</p>
            </div>
            """,
                unsafe_allow_html=True,
            )

        with col_cat2:
            st.markdown(
                f"""
            <div class="stat-box" style="border-left: 4px solid #ffd600;">
                <h3>üü° Kh√°</h3>
                <h2>{stats['good_count']}</h2>
                <p>75-89 ƒëi·ªÉm</p>
            </div>
            """,
                unsafe_allow_html=True,
            )

        with col_cat3:
            st.markdown(
                f"""
            <div class="stat-box" style="border-left: 4px solid #ff9800;">
                <h3>üü† Trung b√¨nh</h3>
                <h2>{stats['average_count']}</h2>
                <p>60-74 ƒëi·ªÉm</p>
            </div>
            """,
                unsafe_allow_html=True,
            )

        with col_cat4:
            st.markdown(
                f"""
            <div class="stat-box" style="border-left: 4px solid #ff5252;">
                <h3>üî¥ C·∫ßn c·∫£i thi·ªán</h3>
                <h2>{stats['poor_count']}</h2>
                <p>< 60 ƒëi·ªÉm</p>
            </div>
            """,
                unsafe_allow_html=True,
            )

        st.divider()

        # Xu·∫•t b√°o c√°o
        st.subheader("üì• Xu·∫•t d·ªØ li·ªáu")

        col_export1, col_export2, col_export3 = st.columns(3)

        with col_export1:
            csv_data = export_history_to_csv()
            if csv_data:
                st.download_button(
                    label="üíæ T·∫£i CSV",
                    data=csv_data,
                    file_name=f"speech_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv",
                    use_container_width=True,
                )

        with col_export2:
            json_data = export_history_to_json()
            if json_data:
                st.download_button(
                    label="üíæ T·∫£i JSON (Backup)",
                    data=json_data,
                    file_name=f"speech_analysis_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    mime="application/json",
                    use_container_width=True,
                )

        with col_export3:
            if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠", use_container_width=True):
                if st.session_state.get("confirm_delete", False):
                    st.session_state.history = []
                    st.session_state.confirm_delete = False
                    st.rerun()
                else:
                    st.session_state.confirm_delete = True
                    st.warning("‚ö†Ô∏è Nh·∫•n l·∫ßn n·ªØa ƒë·ªÉ x√°c nh·∫≠n!")

    else:
        st.info("üìä Ch∆∞a c√≥ d·ªØ li·ªáu th·ªëng k√™. H√£y b·∫Øt ƒë·∫ßu ph√¢n t√≠ch b√†i n√≥i ƒë·∫ßu ti√™n!")

# TAB 3: C√ÄI ƒê·∫∂T
with tab3:
    st.title("‚öôÔ∏è C√†i ƒë·∫∑t")

    # Th√¥ng tin ng∆∞·ªùi d√πng
    st.subheader("üë§ Th√¥ng tin ng∆∞·ªùi d√πng")
    user_name = st.text_input("T√™n c·ªßa b·∫°n:", value=st.session_state.user_name)

    if user_name != st.session_state.user_name:
        st.session_state.user_name = user_name
        st.success("‚úÖ ƒê√£ l∆∞u t√™n!")

    st.divider()

    # C√†i ƒë·∫∑t m·ª•c ti√™u
    st.subheader("üéØ M·ª•c ti√™u luy·ªán t·∫≠p")
    daily_goal = st.slider(
        "S·ªë l·∫ßn ph√¢n t√≠ch m·ªói ng√†y:",
        min_value=1,
        max_value=50,
        value=st.session_state.daily_goal,
        help="ƒê·∫∑t m·ª•c ti√™u s·ªë l·∫ßn ph√¢n t√≠ch b√†i n√≥i h√†ng ng√†y",
    )

    if daily_goal != st.session_state.daily_goal:
        st.session_state.daily_goal = daily_goal
        st.success(f"‚úÖ ƒê√£ ƒë·∫∑t m·ª•c ti√™u: {daily_goal} l·∫ßn/ng√†y")

    st.divider()

    # C√†i ƒë·∫∑t Model Whisper
    st.subheader("ü§ñ M√¥ h√¨nh AI")

    col_model1, col_model2 = st.columns(2)

    with col_model1:
        model_size = st.selectbox(
            "Ch·ªçn m√¥ h√¨nh Whisper:",
            options=["tiny", "base", "small"],
            index=["tiny", "base", "small"].index(st.session_state.model_size),
            help="""
            - tiny: Nhanh nh·∫•t, ƒë·ªô ch√≠nh x√°c th·∫•p nh·∫•t (39MB)
            - base: C√¢n b·∫±ng t·ªëc ƒë·ªô v√† ƒë·ªô ch√≠nh x√°c (74MB) - Khuy·∫øn ngh·ªã
            - small: Ch·∫≠m h∆°n nh∆∞ng ch√≠nh x√°c h∆°n (244MB)
            """,
        )

    with col_model2:
        if st.session_state.model is not None:
            st.success(f"Model ƒëang d√πng: **{st.session_state.model_size}**")
        else:
            st.warning("Ch∆∞a load model")

    if st.button("Load/Reload Model", type="primary", use_container_width=True):
        with st.spinner(f"ƒêang t·∫£i model {model_size}..."):
            model = load_whisper_model(model_size)
            if model:
                st.session_state.model = model
                st.session_state.model_size = model_size
                st.success(f"‚úÖ ƒê√£ load model {model_size} th√†nh c√¥ng!")
                st.balloons()
            else:
                st.error("‚ùå Kh√¥ng th·ªÉ load model. Vui l√≤ng th·ª≠ l·∫°i.")

    st.divider()

    # Import/Export d·ªØ li·ªáu
    st.subheader("üíæ Sao l∆∞u & Kh√¥i ph·ª•c")

    col_backup1, col_backup2 = st.columns(2)

    with col_backup1:
        st.markdown("**Xu·∫•t d·ªØ li·ªáu (Backup)**")
        json_backup = export_history_to_json()
        if json_backup:
            st.download_button(
                label="üì• T·∫£i file backup (JSON)",
                data=json_backup,
                file_name=f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json",
                use_container_width=True,
            )
        else:
            st.info("Ch∆∞a c√≥ d·ªØ li·ªáu ƒë·ªÉ backup")

    with col_backup2:
        st.markdown("**Nh·∫≠p d·ªØ li·ªáu (Restore)**")
        upload_backup = st.file_uploader(
            "Ch·ªçn file backup JSON:",
            type=["json"],
            help="Upload file JSON ƒë√£ backup tr∆∞·ªõc ƒë√≥",
        )

        if upload_backup:
            if st.button("üì§ Kh√¥i ph·ª•c d·ªØ li·ªáu", use_container_width=True):
                if import_history_from_json(upload_backup):
                    st.success("‚úÖ ƒê√£ kh√¥i ph·ª•c d·ªØ li·ªáu th√†nh c√¥ng!")
                    st.rerun()
                else:
                    st.error("‚ùå Kh√¥ng th·ªÉ kh√¥i ph·ª•c. File kh√¥ng h·ª£p l·ªá.")

    st.divider()

    # Th√¥ng tin ·ª©ng d·ª•ng
    st.subheader("‚ÑπÔ∏è Th√¥ng tin ·ª©ng d·ª•ng")

    st.info(
        """
    **üé§ ·ª®ng d·ª•ng Ph√¢n t√≠ch Ph√°t √Çm B√†i N√≥i T·ª± Do**
    
    **Phi√™n b·∫£n:** 3.1 - Enhanced Pronunciation Feedback
    
    **T√≠nh nƒÉng ch√≠nh:**
    - Ph√¢n t√≠ch ph√°t √¢m t·ª± ƒë·ªông v·ªõi AI (Whisper)
    - Ch·∫•m ƒëi·ªÉm theo 5 ti√™u ch√≠ v·ªõi ph·∫£n h·ªìi chi ti·∫øt
    - Ph√°t hi·ªán t·ª´ ph√°t √¢m sai v√† ƒë∆∞a ra g·ª£i √Ω s·ª≠a
    - Th·ªëng k√™ chi ti·∫øt v√† bi·ªÉu ƒë·ªì tr·ª±c quan
    - Theo d√µi streak v√† m·ª•c ti√™u h√†ng ng√†y
    - Sao l∆∞u/kh√¥i ph·ª•c d·ªØ li·ªáu
    
    **C√¥ng ngh·ªá:**
    - Streamlit
    - OpenAI Whisper (ASR)
    - Plotly Charts
    - Advanced NLP Analysis
    
    **Thang ƒëi·ªÉm:**
    - Pronunciation: ƒê·ªô r√µ r√†ng, ch√≠nh x√°c ph√°t √¢m (/2)
    - Fluency: ƒê·ªô tr√¥i ch·∫£y, t·ª± nhi√™n (/2)
    - Grammar: Ng·ªØ ph√°p ch√≠nh x√°c (/2)
    - Vocabulary: T·ª´ v·ª±ng ƒëa d·∫°ng (/2)
    - Communication: Truy·ªÅn ƒë·∫°t √Ω r√µ r√†ng (/2)
    - T·ªïng: /10 ƒëi·ªÉm (t∆∞∆°ng ƒë∆∞∆°ng /100)
    """
    )

    st.divider()

    # N√∫t reset to√†n b·ªô
    st.subheader("üö® Zone Nguy hi·ªÉm")

    if st.button("üóëÔ∏è X√≥a to√†n b·ªô d·ªØ li·ªáu v√† c√†i ƒë·∫∑t", type="secondary"):
        if st.session_state.get("confirm_reset_all", False):
            st.session_state.history = []
            st.session_state.user_name = "H·ªçc vi√™n"
            st.session_state.daily_goal = 10
            st.session_state.confirm_reset_all = False
            st.success("‚úÖ ƒê√£ reset to√†n b·ªô!")
            st.rerun()
        else:
            st.session_state.confirm_reset_all = True
            st.error(
                "‚ö†Ô∏è C·∫¢NH B√ÅO: B·∫°n s·∫Ω m·∫•t to√†n b·ªô d·ªØ li·ªáu! Nh·∫•n l·∫ßn n·ªØa ƒë·ªÉ x√°c nh·∫≠n."
            )

# Footer
st.divider()
st.markdown(
    """
<style>
.footer-link {
    color: gray;
    text-decoration: none;
    transition: color 0.3s ease;
}
.footer-link:hover {
    color: #1f77b4;
}
</style>

<div style='text-align: center; color: gray; padding: 16px 0; font-size: 14px;'>
    <p><strong>·ª®ng d·ª•ng Ph√¢n t√≠ch Ph√°t √Çm B√†i N√≥i T·ª± Do</strong></p>
    <p>Mi·ªÖn ph√≠ ‚Ä¢ Offline/Online ‚Ä¢ Kh√¥ng d√πng API tr·∫£ ph√≠</p>
    <p>S·ª≠ d·ª•ng: Whisper (OpenAI) + Streamlit + Enhanced Pronunciation Analysis</p>
    <p style='margin-top:10px;'>¬© 2025 
        <a href='https://www.facebook.com/augusttrung1823/' target='_blank' class='footer-link'>
            August Trung
        </a>. All rights reserved.
    </p>
</div>
""",
    unsafe_allow_html=True,
)
